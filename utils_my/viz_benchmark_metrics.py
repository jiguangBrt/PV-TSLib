import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
PROJECT_ROOT = "/root/autodl-tmp/Time-Series-Library/"
RESULTS_ROOT = os.path.join(PROJECT_ROOT, "results/")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "dataset/viz_results/")

# æ¨¡å‹é¡ºåº (PhysicsMamba æ”¾æœ€å‰)
MODEL_ORDER = [
    'PhysicsMamba',
    'iTransformer', 
    'PatchTST', 
    'Mamba', 
    'Transformer', 
    'Informer', 
    'Autoformer'
]

# å¼‚å¸¸å€¼é˜ˆå€¼ (MSE > 2.0 è§†ä¸ºè®­ç»ƒå¤±è´¥)
MSE_THRESHOLD = 2.0 

# é¢œè‰²é…ç½®
COLORS = {
    'PhysicsMamba': '#E74C3C',    # çº¢è‰² (ä¸»è§’)
    'iTransformer': '#3498DB',    # è“è‰²
    'PatchTST': '#2ECC71',        # ç»¿è‰²
    'Mamba': '#F39C12',           # æ©™è‰²
    'Transformer': '#9B59B6',     # ç´«è‰²
    'Informer': '#1ABC9C',        # é’è‰²
    'Autoformer': '#95A5A6'       # ç°è‰²
}

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ================= ğŸ“Š æ•°æ®æå–å‡½æ•° =================
def extract_all_metrics():
    """
    æå–æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡:
    - MSE/MAE (ä» metrics.npy)
    - ï¿½ï¿½ç»ƒæ—¶é—´ã€æ¨ç†æ—¶é—´ã€FLOPs (ä» performance.npy)
    """
    records = []
    exp_dirs = [d for d in os.listdir(RESULTS_ROOT) if os.path.isdir(os.path.join(RESULTS_ROOT, d))]
    print(f"ğŸ” Found {len(exp_dirs)} experiment records in {RESULTS_ROOT}\n")
    
    for folder_name in exp_dirs:
        try:
            # Step 1: æå– Horizon
            if 'Short24' in folder_name:
                horizon = 'Short Term (24)'
                tag = 'Short24'
            elif 'Long96' in folder_name:
                horizon = 'Long Term (96)'
                tag = 'Long96'
            else:
                continue
            
            # Step 2: æå–æ¨¡å‹å
            model_name = "Unknown"
            for m in MODEL_ORDER:
                if m in folder_name:
                    model_name = m
                    break
            if model_name == "Unknown":
                continue
            
            # Step 3: æå– Site
            start_marker = 'forecast_'
            end_marker = f'_{tag}'
            start_idx = folder_name.find(start_marker)
            if start_idx == -1:
                continue
            start_idx += len(start_marker)
            end_idx = folder_name.find(end_marker, start_idx)
            if end_idx == -1:
                continue
            site_name = folder_name[start_idx:end_idx]
            
            # Step 4: è¯»å–åŸºç¡€æŒ‡æ ‡ (metrics.npy)
            metric_path = os.path.join(RESULTS_ROOT, folder_name, 'metrics.npy')
            if not os.path.exists(metric_path):
                continue
            metrics = np.load(metric_path)
            mse = float(metrics[1])
            mae = float(metrics[0])
            
            # Step 5: è¯»å–æ€§èƒ½æŒ‡æ ‡ (performance.npy) - æ–¹æ¡ˆ A ä¿®æ”¹åçš„è¾“å‡º
            perf_path = os.path.join(RESULTS_ROOT, folder_name, 'performance.npy')
            if os.path.exists(perf_path):
                try:
                    perf = np.load(perf_path, allow_pickle=True).item()
                    train_time = perf.get('total_train_time', None)
                    inference_time = perf.get('avg_inference_time', None)
                    flops = perf.get('flops', 'N/A')
                    params = perf.get('params', 'N/A')
                except:
                    train_time = None
                    inference_time = None
                    flops = 'N/A'
                    params = 'N/A'
            else:
                train_time = None
                inference_time = None
                flops = 'N/A'
                params = 'N/A'
            
            records.append({
                'Site': site_name,
                'Model': model_name,
                'Horizon': horizon,
                'MSE': mse,
                'MAE': mae,
                'Train Time (s)': train_time,
                'Inference Time (ms)': inference_time * 1000 if inference_time else None,
                'FLOPs': str(flops),
                'Params': str(params)
            })
            
            print(f"âœ… {site_name:20s} | {model_name:15s} | {horizon:18s} | MSE={mse:.4f}")
            
        except Exception as e:
            print(f"âš ï¸ Failed: {folder_name[:50]}... - {e}")
            continue
    
    df = pd.DataFrame(records)
    print(f"\nğŸ“Š Total valid records: {len(df)}")
    return df

# ================= ğŸ¨ å¯è§†åŒ–å‡½æ•° =================
def plot_mse_comparison(df):
    """ç»˜åˆ¶ MSE å¯¹æ¯”æŸ±çŠ¶å›¾"""
    df_clean = df[df['MSE'] < MSE_THRESHOLD].copy()
    
    sns.set_theme(style="whitegrid", font_scale=1.1)
    horizons = sorted(df_clean['Horizon'].unique())
    
    for horizon in horizons:
        subset = df_clean[df_clean['Horizon'] == horizon].copy()
        subset = subset.sort_values(by=['Site', 'Model'])
        
        y_max = subset['MSE'].max() * 1.2
        
        fig, ax = plt.subplots(figsize=(20, 10))
        
        sns.barplot(
            data=subset,
            x='Site',
            y='MSE',
            hue='Model',
            hue_order=MODEL_ORDER,
            palette=COLORS,
            edgecolor="black",
            linewidth=0.6,
            ax=ax
        )
        
        # æ ‡æ³¨æ•°å€¼
        for container in ax.containers:
            ax.bar_label(container, fmt='%.3f', padding=3, fontsize=8, rotation=90)
        
        ax.set_ylim(0, y_max)
        plt.title(
            f"MSE Comparison - {horizon}\n(Outliers > {MSE_THRESHOLD} removed)", 
            fontsize=18, fontweight='bold', pad=20
        )
        plt.xlabel("Site", fontsize=14, fontweight='bold')
        plt.ylabel("MSE (Lower is Better)", fontsize=14, fontweight='bold')
        plt.xticks(rotation=35, ha='right', fontsize=11)
        plt.legend(title='Model', bbox_to_anchor=(1.01, 1), loc='upper left', fontsize=11)
        
        ax.yaxis.grid(True, linestyle='--', alpha=0.3)
        ax.set_axisbelow(True)
        plt.tight_layout()
        
        save_name = f"MSE_Comparison_{horizon.replace(' ', '_').replace('(', '').replace(')', '')}.png"
        save_path = os.path.join(OUTPUT_DIR, save_name)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… Saved: {save_path}")
        plt.close()

def plot_inference_time(df):
    """ç»˜åˆ¶æ¨ç†æ—¶é—´å¯¹æ¯”"""
    df_time = df[df['Inference Time (ms)'].notna()].copy()
    if df_time.empty:
        print("âš ï¸ No inference time data found. Skipping...")
        return
    
    # æŒ‰æ¨¡å‹èšåˆå¹³å‡æ¨ç†æ—¶é—´
    df_avg = df_time.groupby('Model')['Inference Time (ms)'].mean().reset_index()
    df_avg = df_avg.sort_values('Inference Time (ms)')
    
    fig, ax = plt.subplots(figsize=(12, 6))
    bars = ax.barh(df_avg['Model'], df_avg['Inference Time (ms)'], 
                   color=[COLORS.get(m, '#95A5A6') for m in df_avg['Model']],
                   edgecolor='black', linewidth=0.8)
    
    # æ ‡æ³¨æ•°å€¼
    for bar in bars:
        width = bar.get_width()
        ax.text(width + 0.5, bar.get_y() + bar.get_height()/2, 
                f'{width:.2f} ms', ha='left', va='center', fontsize=10, fontweight='bold')
    
    plt.xlabel('Average Inference Time (ms)', fontsize=12, fontweight='bold')
    plt.ylabel('Model', fontsize=12, fontweight='bold')
    plt.title('Inference Time Comparison (Avg per Batch)', fontsize=14, fontweight='bold', pad=15)
    plt.grid(axis='x', linestyle='--', alpha=0.3)
    plt.tight_layout()
    
    save_path = os.path.join(OUTPUT_DIR, 'Inference_Time_Comparison.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: {save_path}")
    plt.close()

def plot_train_time(df):
    """ç»˜åˆ¶è®­ç»ƒæ—¶é—´å¯¹æ¯”"""
    df_time = df[df['Train Time (s)'].notna()].copy()
    if df_time.empty:
        print("âš ï¸ No training time data found. Skipping...")
        return
    
    # æŒ‰æ¨¡å‹èšåˆå¹³å‡è®­ç»ƒæ—¶é—´
    df_avg = df_time.groupby('Model')['Train Time (s)'].mean().reset_index()
    df_avg['Train Time (min)'] = df_avg['Train Time (s)'] / 60
    df_avg = df_avg.sort_values('Train Time (min)')
    
    fig, ax = plt.subplots(figsize=(12, 6))
    bars = ax.barh(df_avg['Model'], df_avg['Train Time (min)'], 
                   color=[COLORS.get(m, '#95A5A6') for m in df_avg['Model']],
                   edgecolor='black', linewidth=0.8)
    
    for bar in bars:
        width = bar.get_width()
        ax.text(width + 0.5, bar.get_y() + bar.get_height()/2, 
                f'{width:.1f} min', ha='left', va='center', fontsize=10, fontweight='bold')
    
    plt.xlabel('Average Training Time (minutes)', fontsize=12, fontweight='bold')
    plt.ylabel('Model', fontsize=12, fontweight='bold')
    plt.title('Training Time Comparison', fontsize=14, fontweight='bold', pad=15)
    plt.grid(axis='x', linestyle='--', alpha=0.3)
    plt.tight_layout()
    
    save_path = os.path.join(OUTPUT_DIR, 'Training_Time_Comparison.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: {save_path}")
    plt.close()

def plot_comprehensive_dashboard(df):
    """ç»¼åˆä»ªè¡¨ç›˜: MSE + æ—¶é—´ + FLOPs"""
    df_clean = df[df['MSE'] < MSE_THRESHOLD].copy()
    
    # æŒ‰æ¨¡å‹èšåˆå¹³å‡å€¼
    agg_dict = {
        'MSE': 'mean',
        'MAE': 'mean',
        'Inference Time (ms)': 'mean',
        'Train Time (s)': 'mean'
    }
    df_summary = df_clean.groupby('Model').agg(agg_dict).reset_index()
    df_summary = df_summary.sort_values('MSE')
    
    # åˆ›å»º 2x2 å­å›¾
    fig = plt.figure(figsize=(18, 12))
    gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)
    
    # å­å›¾ 1: MSE
    ax1 = fig.add_subplot(gs[0, 0])
    bars1 = ax1.bar(df_summary['Model'], df_summary['MSE'], 
                    color=[COLORS.get(m, '#95A5A6') for m in df_summary['Model']],
                    edgecolor='black', linewidth=0.8)
    ax1.set_title('Average MSE', fontsize=14, fontweight='bold')
    ax1.set_ylabel('MSE', fontsize=12)
    ax1.tick_params(axis='x', rotation=45)
    ax1.grid(axis='y', linestyle='--', alpha=0.3)
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2, height + 0.01, 
                f'{height:.3f}', ha='center', va='bottom', fontsize=9)
    
    # å­å›¾ 2: MAE
    ax2 = fig.add_subplot(gs[0, 1])
    bars2 = ax2.bar(df_summary['Model'], df_summary['MAE'], 
                    color=[COLORS.get(m, '#95A5A6') for m in df_summary['Model']],
                    edgecolor='black', linewidth=0.8)
    ax2.set_title('Average MAE', fontsize=14, fontweight='bold')
    ax2.set_ylabel('MAE', fontsize=12)
    ax2.tick_params(axis='x', rotation=45)
    ax2.grid(axis='y', linestyle='--', alpha=0.3)
    for bar in bars2:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2, height + 0.005, 
                f'{height:.3f}', ha='center', va='bottom', fontsize=9)
    
    # å­å›¾ 3: æ¨ç†æ—¶é—´
    ax3 = fig.add_subplot(gs[1, 0])
    df_time = df_summary[df_summary['Inference Time (ms)'].notna()]
    if not df_time.empty:
        bars3 = ax3.bar(df_time['Model'], df_time['Inference Time (ms)'], 
                        color=[COLORS.get(m, '#95A5A6') for m in df_time['Model']],
                        edgecolor='black', linewidth=0.8)
        ax3.set_title('Avg Inference Time', fontsize=14, fontweight='bold')
        ax3.set_ylabel('Time (ms)', fontsize=12)
        ax3.tick_params(axis='x', rotation=45)
        ax3.grid(axis='y', linestyle='--', alpha=0.3)
        for bar in bars3:
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2, height + 0.5, 
                    f'{height:.1f}', ha='center', va='bottom', fontsize=9)
    
    # å­å›¾ 4: è®­ç»ƒæ—¶é—´
    ax4 = fig.add_subplot(gs[1, 1])
    df_train = df_summary[df_summary['Train Time (s)'].notna()].copy()
    if not df_train.empty:
        df_train['Train Time (min)'] = df_train['Train Time (s)'] / 60
        bars4 = ax4.bar(df_train['Model'], df_train['Train Time (min)'], 
                        color=[COLORS.get(m, '#95A5A6') for m in df_train['Model']],
                        edgecolor='black', linewidth=0.8)
        ax4.set_title('Avg Training Time', fontsize=14, fontweight='bold')
        ax4.set_ylabel('Time (minutes)', fontsize=12)
        ax4.tick_params(axis='x', rotation=45)
        ax4.grid(axis='y', linestyle='--', alpha=0.3)
        for bar in bars4:
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2, height + 1, 
                    f'{height:.1f}', ha='center', va='bottom', fontsize=9)
    
    plt.suptitle('Comprehensive Performance Dashboard', fontsize=18, fontweight='bold', y=0.98)
    
    save_path = os.path.join(OUTPUT_DIR, 'Comprehensive_Dashboard.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: {save_path}")
    plt.close()

def export_summary_table(df):
    """å¯¼å‡º FLOPs å’Œå‚æ•°é‡æ±‡æ€»è¡¨"""
    # æå–æ¯ä¸ªæ¨¡å‹çš„ FLOPs å’Œ Params (å»é‡)
    flops_data = df[['Model', 'FLOPs', 'Params']].drop_duplicates(subset=['Model'])
    
    # è®¡ç®—å¹³å‡æ€§èƒ½æŒ‡æ ‡
    df_clean = df[df['MSE'] < MSE_THRESHOLD].copy()
    perf_summary = df_clean.groupby('Model').agg({
        'MSE': 'mean',
        'MAE': 'mean',
        'Inference Time (ms)': 'mean',
        'Train Time (s)': lambda x: x.mean() / 60  # è½¬ä¸ºåˆ†é’Ÿ
    }).reset_index()
    
    # åˆå¹¶
    summary = perf_summary.merge(flops_data, on='Model', how='left')
    summary = summary.rename(columns={'Train Time (s)': 'Train Time (min)'})
    
    # æ’åº
    summary = summary.sort_values('MSE')
    
    # ä¿å­˜ CSV
    csv_path = os.path.join(OUTPUT_DIR, 'Performance_Summary.csv')
    summary.to_csv(csv_path, index=False, float_format='%.4f')
    print(f"\nğŸ“Š Performance summary saved to: {csv_path}")
    
    # æ‰“å°è¡¨æ ¼
    print("\n" + "="*100)
    print("Model Performance Summary".center(100))
    print("="*100)
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', 120)
    print(summary.to_string(index=False))
    print("="*100 + "\n")

def compute_improvement(df):
    """è®¡ç®— PhysicsMamba ç›¸å¯¹æœ€ä½³ Baseline çš„æ”¹è¿›"""
    if 'PhysicsMamba' not in df['Model'].values:
        print("âš ï¸ No PhysicsMamba results found.")
        return
    
    df_clean = df[df['MSE'] < MSE_THRESHOLD].copy()
    
    results = []
    for horizon in df_clean['Horizon'].unique():
        for site in df_clean['Site'].unique():
            pm_data = df_clean[(df_clean['Model'] == 'PhysicsMamba') & 
                               (df_clean['Horizon'] == horizon) & 
                               (df_clean['Site'] == site)]
            if pm_data.empty:
                continue
            pm_mse = pm_data['MSE'].values[0]
            
            baselines = df_clean[(df_clean['Model'] != 'PhysicsMamba') & 
                                 (df_clean['Horizon'] == horizon) & 
                                 (df_clean['Site'] == site)]
            if baselines.empty:
                continue
            
            best_baseline = baselines.loc[baselines['MSE'].idxmin()]
            best_mse = best_baseline['MSE']
            best_model = best_baseline['Model']
            
            improvement = ((best_mse - pm_mse) / best_mse) * 100
            
            results.append({
                'Site': site,
                'Horizon': horizon,
                'PhysicsMamba MSE': pm_mse,
                'Best Baseline': best_model,
                'Baseline MSE': best_mse,
                'Improvement (%)': improvement
            })
    
    result_df = pd.DataFrame(results)
    csv_path = os.path.join(OUTPUT_DIR, "PhysicsMamba_Improvement.csv")
    result_df.to_csv(csv_path, index=False)
    
    print("\n" + "="*80)
    print("PhysicsMamba vs Best Baseline".center(80))
    print("="*80)
    print(result_df.to_string(index=False))
    print("="*80)
    print(f"Average Improvement: {result_df['Improvement (%)'].mean():.2f}%")
    print(f"Wins: {len(result_df[result_df['Improvement (%)'] > 0])} / {len(result_df)}")
    print("="*80 + "\n")

# ================= ğŸš€ ä¸»å‡½æ•° =================
if __name__ == "__main__":
    print("ğŸ” Extracting all metrics from results folder...\n")
    df = extract_all_metrics()
    
    if df.empty:
        print("âŒ No data extracted. Check your results folder!")
    else:
        print(f"\nâœ… Extracted {len(df)} records.")
        
        # 1. MSE å¯¹æ¯”å›¾
        print("\nğŸ¨ Generating MSE comparison charts...")
        plot_mse_comparison(df)
        
        # 2. æ¨ç†æ—¶é—´å¯¹æ¯”
        print("\nâ±ï¸ Generating inference time comparison...")
        plot_inference_time(df)
        
        # 3. è®­ç»ƒæ—¶é—´å¯¹æ¯”
        print("\nâ±ï¸ Generating training time comparison...")
        plot_train_time(df)
        
        # 4. ç»¼åˆä»ªè¡¨ç›˜
        print("\nğŸ“Š Generating comprehensive dashboard...")
        plot_comprehensive_dashboard(df)
        
        # 5. å¯¼å‡ºæ±‡æ€»è¡¨
        print("\nğŸ“‹ Exporting summary table...")
        export_summary_table(df)
        
        # 6. è®¡ç®—æ”¹è¿›ç‡
        print("\nğŸ“ˆ Computing improvement over baselines...")
        compute_improvement(df)
        
        print("\nğŸ‰ All visualizations complete!")
        print(f"ğŸ“ Results saved to: {OUTPUT_DIR}")