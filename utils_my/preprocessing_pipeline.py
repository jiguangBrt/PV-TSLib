import pandas as pd
import numpy as np
from vmdpy import VMD
import pvlib
from tqdm import tqdm
import multiprocessing
import os
import glob

# --- ğŸ“ è·¯å¾„é…ç½® ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
INPUT_DIR = os.path.join(BASE_DIR, '../dataset/solar_raw_clean') # è¯»å–æ¸…æ´—åçš„æ•°æ®
OUTPUT_DIR = os.path.join(BASE_DIR, '../dataset/solar_processed_mvmd')

# --- âš™ï¸ VMD å‚æ•°é…ç½® ---
K_MODES = 8
ALPHA = 2000
TAU = 0

# --- ğŸŒ ç‰©ç†å‚æ•° ---
DEFAULT_LAT = 37.0
DEFAULT_LON = 112.0

def calc_physics_baseline(df, lat=DEFAULT_LAT, lon=DEFAULT_LON):
    try:
        times = pd.to_datetime(df['date'])
        if times.dt.tz is None:
            times = pd.DatetimeIndex(times).tz_localize('UTC')
        else:
            times = pd.DatetimeIndex(times)

        location = pvlib.location.Location(lat, lon)
        cs = location.get_clearsky(times)
        
        ghi = cs['ghi'].values
        real_power = df['OT'].values
        
        valid_mask = ghi > 10 
        if np.sum(valid_mask) > 0:
            ratio = np.percentile(real_power[valid_mask], 95) / np.percentile(ghi[valid_mask], 95)
        else:
            ratio = 1.0
            
        p_phy = ghi * ratio
        p_phy = np.nan_to_num(p_phy, nan=0.0) 
        if len(p_phy) != len(df): p_phy = p_phy[:len(df)]
        return p_phy

    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"âš ï¸ Physics calc failed: {e}. Using zeros.")
        return np.zeros(len(df))
    
def run_vmd(signal):
    if np.all(signal == signal[0]):
        return np.zeros((len(signal), K_MODES))
    try:
        u, _, _ = VMD(signal, ALPHA, TAU, K_MODES, 0, 1, 1e-7)
        return u.T
    except Exception as e:
        print(f"âš ï¸ VMD failed: {e}. Return zeros.")
        return np.zeros((len(signal), K_MODES))

def process_single_file(file_path):
    filename = os.path.basename(file_path)
    save_path = os.path.join(OUTPUT_DIR, filename)
    
    print(f"\nğŸ“„ Processing: {filename} ...")
    
    # 1. è¯»å–æ•°æ® (å·²ç»æ˜¯æ ‡å‡†åŒ–çš„ clean æ•°æ®)
    try:
        df = pd.read_csv(file_path)
    except Exception as e:
        print(f"âŒ Read failed: {e}")
        return 0
    
    # 2. ç‰©ç†è®¡ç®—
    if 'OT' not in df.columns:
        print(f"âŒ Skipping {filename}: No 'OT' column found.")
        return 0
        
    p_raw = df['OT'].values
    p_phy = calc_physics_baseline(df)
    p_res = p_raw - p_phy
    
    # 3. å‡†å¤‡ MVMD è¾“å…¥ (åŠ¨æ€é€‰æ‹©å­˜åœ¨çš„åˆ—)
    targets = {}
    
    # (a) æ ¸å¿ƒå˜é‡
    targets['PowerRes'] = p_res
    
    # (b) è¾…åŠ©å˜é‡ (å¦‚æœå­˜åœ¨æ‰æ·»åŠ )
    if 'GHI' in df.columns:
        targets['GHI'] = df['GHI'].values
    if 'Temp' in df.columns:
        targets['Temp'] = df['Temp'].values
    if 'Humid' in df.columns:
        targets['Humid'] = df['Humid'].values
    if 'Pressure' in df.columns:
        targets['Pressure'] = df['Pressure'].values
    # å¦‚æœæœ‰ DNI/TSI ä¹Ÿå¯ä»¥åŠ ï¼Œæ ¹æ®ä½ çš„éœ€æ±‚
    
    # --- æ£€æŸ¥ NaN/Inf ---
    for k, v in targets.items():
        if not np.isfinite(v).all():
            print(f"âš ï¸ Warning: {k} contains NaN/Inf. Filling with 0.")
            targets[k] = np.nan_to_num(v, nan=0.0, posinf=0.0, neginf=0.0)

    # 4. æ‰§è¡Œå¤šè¿›ç¨‹ VMD
    pool = multiprocessing.Pool(processes=len(targets))
    results = []
    keys = []
    
    for key, signal in targets.items():
        keys.append(key)
        results.append(pool.apply_async(run_vmd, (signal,)))
    
    pool.close()
    pool.join()
    
    # 5. ç»„è£…ç»“æœ
    df_out = pd.DataFrame()
    df_out['date'] = df['date']
    df_out['OT'] = p_raw      # çœŸå€¼
    df_out['P_PHY'] = p_phy   # ç‰©ç†åŸºçº¿
    
    # æ”¾å…¥åˆ†è§£åçš„åˆ†é‡
    for key, res in zip(keys, results):
        modes = res.get() # [L, K]
        for k in range(K_MODES):
            col_name = f'{key}_IMF{k+1}'
            df_out[col_name] = modes[:, k]
            
    # 6. ä¿å­˜
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    
    df_out.to_csv(save_path, index=False)
    print(f"âœ… Saved to: {save_path}")
    print(f"   Decomposed {len(targets)} variables -> {len(df_out.columns)-3} features.")
    
    # è¿”å›ç‰¹å¾æ•°é‡ (å‡å» date, OT, P_PHY è¿™3ä¸ªéè¾“å…¥åˆ—)
    feature_count = len(df_out.columns) - 3 
    return feature_count

if __name__ == '__main__':
    csv_files = glob.glob(os.path.join(INPUT_DIR, "*.csv"))
    
    if not csv_files:
        print(f"âŒ No CSV files found in {INPUT_DIR}")
        exit()
        
    print(f"ğŸ” Found {len(csv_files)} files. Starting pipeline...")
    
    # æˆ‘ä»¬å‡è®¾æ‰€æœ‰æ–‡ä»¶çš„ç‰¹å¾ç»´åº¦æ˜¯ä¸€æ ·çš„ï¼Œæˆ–è€…å–ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„ç»´åº¦ä½œä¸ºå‚è€ƒ
    # å¦‚æœæ¯ä¸ªæ–‡ä»¶å› ä¸ºç¼ºå¤±åˆ—å¯¼è‡´ç‰¹å¾ç»´åº¦ä¸åŒï¼Œè®­ç»ƒæ—¶ä¼šæœ‰éº»çƒ¦ï¼ˆEnc_in ä¸åŒ¹é…ï¼‰
    # ä½†æŒ‰ç…§ç›®å‰çš„æ¸…æ´—è„šæœ¬ï¼Œå¤§éƒ¨åˆ†ä¸»è¦åˆ—åº”è¯¥éƒ½åœ¨ã€‚
    
    enc_in_list = []
    for f in csv_files:
        feat_dim = process_single_file(f)
        enc_in_list.append(feat_dim)
        
    # æ£€æŸ¥ç»´åº¦ä¸€è‡´æ€§
    if len(set(enc_in_list)) > 1:
        print("\nâš ï¸ WARNING: Feature dimensions inconsistent across files!")
        print(f"Dimensions found: {enc_in_list}")
        print("Model training might fail if batch_train mixes these files.")
    
    final_dim = enc_in_list[0] if enc_in_list else 0
    
    print("\n" + "="*50)
    print("ğŸš€ All Done!")
    print(f"âš ï¸  Please use this for your run.py settings:")
    print(f"   --enc_in {final_dim} --c_out {final_dim}")
    print("="*50)